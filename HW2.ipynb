{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time\n",
    "import bs4\n",
    "import base64\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import locale\n",
    "import pymystem3 as ms\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_index_name=\"raw_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index(name, settings):\n",
    "    if es.indices.exists(index=name):\n",
    "        es.indices.delete(index=name)\n",
    "    es.indices.create(index=name, body=settings)\n",
    "    \n",
    "def check_analyzer(raw_index_name, analyzer, text):\n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    \n",
    "    tokens = es.indices.analyze(index=raw_index_name, body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_settings = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'id': {\n",
    "                'type': 'integer'\n",
    "            },\n",
    "            'url': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                'fields': {\n",
    "                    'complex': {\n",
    "                        'type': 'text',\n",
    "                        'analyzer': 'russian_complex'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'russian_complex': {\n",
    "                    'char_filter': [\n",
    "                        'no_html'\n",
    "                    ],\n",
    "                    'tokenizer': 'word_longer_2',\n",
    "                    'filter': [\n",
    "                        'lowercase',\n",
    "                        'russian_snow'\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "            'char_filter': {\n",
    "                'no_html': {\n",
    "                    'type': 'html_strip',\n",
    "                    \"escaped_tags\": []\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'word_longer_2': {\n",
    "                    'type': 'pattern',\n",
    "                    'pattern': '[a-zA-Z_0-9\\u0400-\\u04FF]{2,}',\n",
    "                    'group': 0\n",
    "                }\n",
    "            },\n",
    "            'filter': {\n",
    "                'russian_snow': {\n",
    "                    'type': 'snowball',\n",
    "                    'language': 'russian'\n",
    "                }    \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "no_snowball_settings = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'id': {\n",
    "                'type': 'integer'\n",
    "            },\n",
    "            'url': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                'fields': {\n",
    "                    'complex': {\n",
    "                        'type': 'text',\n",
    "                        'analyzer': 'russian_complex'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'russian_complex': {\n",
    "                    'char_filter': [\n",
    "                        'no_html'\n",
    "                    ],\n",
    "                    'tokenizer': 'word_longer_2',\n",
    "                    'filter': [\n",
    "                        'lowercase'\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "            'char_filter': {\n",
    "                'no_html': {\n",
    "                    'type': 'html_strip',\n",
    "                    \"escaped_tags\": []\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'word_longer_2': {\n",
    "                    'type': 'pattern',\n",
    "                    'pattern': '[a-zA-Z_0-9\\u0400-\\u04FF]{2,}',\n",
    "                    'group': 0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_snowball = 'raw_snowball'\n",
    "pre_no_snowball = 'pre_no_snowball'\n",
    "pre_snowball = 'pre_snowball'\n",
    "lemma_no_snowball = 'lemma_no_snowball'\n",
    "\n",
    "def recreate_all():\n",
    "    recreate_index(raw_snowball, snowball_settings)\n",
    "    recreate_index(pre_no_snowball, no_snowball_settings)\n",
    "    recreate_index(pre_snowball, snowball_settings)\n",
    "    recreate_index(lemma_no_snowball, no_snowball_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_generator(index_name):\n",
    "    index = 0\n",
    "    doc = {}\n",
    "    for doc_id in range(10):\n",
    "        with open(f'../byweb_for_course/byweb.{doc_id}.xml', 'r') as inf:\n",
    "            inf.readline()\n",
    "            for i in tqdm(range(20000)):\n",
    "                index = index + 1\n",
    "                a = inf.readline()\n",
    "                b = inf.readline()\n",
    "                c = inf.readline()\n",
    "                doc['content'] = base64.b64decode(re.split('<.*?>', a)[2]).decode(\"cp1251\")\n",
    "                doc['url'] = base64.b64decode(re.split('<.*?>', b)[1]).decode(\"cp1251\")\n",
    "                doc['id'] = re.split('<.*?>', c)[1]\n",
    "                yield create_es_action(index_name, index, doc)\n",
    "                \n",
    "def preprocessing_generator(index_name):\n",
    "    index = 0\n",
    "    doc = {}\n",
    "    with open('docs.out', 'r') as inf:\n",
    "        for i in tqdm(range(200000)):\n",
    "            index = index + 1\n",
    "            s = inf.readline().split('\\t')\n",
    "            doc['content'] = s[2]\n",
    "            doc['url'] = s[1]\n",
    "            doc['id'] = s[0]\n",
    "            yield create_es_action(index_name, index, doc)\n",
    "            \n",
    "def lemmas_generator(index_name):\n",
    "    index = 0\n",
    "    doc = {}\n",
    "    with open('lemmas.out', 'r') as inf:\n",
    "        for i in tqdm(range(200000)):\n",
    "            index = index + 1\n",
    "            s = inf.readline().split('\\t')\n",
    "            doc['content'] = s[2]\n",
    "            doc['url'] = s[1]\n",
    "            doc['id'] = s[0]\n",
    "            yield create_es_action(index_name, index, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generator(index_name, es_actions_generator):\n",
    "    for ok, result in parallel_bulk(es, es_actions_generator(index_name), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "        if not ok:\n",
    "            print('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(index, query, *args):\n",
    "    pretty_print_result(es.search(index=index, body=query, size=20), args)\n",
    "    # note that size set to 20 just because default value is 10 and we know that we have 12 docs and 10 < 12 < 20\n",
    "                        \n",
    "def pretty_print_result(search_result, fields=[]):\n",
    "    # fields is a list of fields names which we want to be printed\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "        for field in fields:\n",
    "            print(f'{field}: {hit[\"_source\"][field]}')\n",
    "                  \n",
    "def get_doc_by_id(index, doc_id):\n",
    "    return es.get(index=index, id=doc_id)['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = open('../web2008_adhoc.xml', 'r', encoding='cp1251')\n",
    "for i in range(9):\n",
    "    inf.readline()\n",
    "    \n",
    "queries = dict()\n",
    "for s in inf.readlines()[:-1]:\n",
    "    query_text = re.split('<.*?>', s)[2]\n",
    "    query_id = s[10:18]\n",
    "    queries[query_id] = query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_docid(index, query_id):\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'must': {\n",
    "                    'match': {\n",
    "                        'content': queries[query_id]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index, body=query, size=300)\n",
    "    hits = res['hits']['hits']\n",
    "    return [doc['_source']['id'] for doc in hits]\n",
    "    \n",
    "def precision(k, ids, rels):\n",
    "    ids = ids[:k]\n",
    "    if len(ids) == 0:\n",
    "        return 0\n",
    "    top = 0\n",
    "    good = 0\n",
    "    for i in ids:\n",
    "        if i in rels:\n",
    "            top += 1\n",
    "            good += rels[i]\n",
    "    if top == 0:\n",
    "        return 0\n",
    "    return good / top\n",
    "\n",
    "def recall(k, ids, rels):\n",
    "    ids = ids[:k]\n",
    "    if len(ids) == 0:\n",
    "        return 0\n",
    "    good = 0\n",
    "    num_good = np.sum(list(rels.values()))\n",
    "    relev = min(num_good, k)\n",
    "    for i in ids:\n",
    "        if i in rels:\n",
    "            good += rels[i]\n",
    "    if relev == 0:\n",
    "        return 0\n",
    "    return good / relev\n",
    "\n",
    "def mean_ap(k, ids, rels):\n",
    "    ids = ids[:k]\n",
    "    ps = []\n",
    "    for i in ids:\n",
    "        if i in rels:\n",
    "            ps.append(rels[i])\n",
    "    if np.sum(ps) == 0:\n",
    "        return 0\n",
    "    sums = np.cumsum(ps) / (np.array(range(len(ps))) + 1)\n",
    "    return np.sum(sums * ps) / np.sum(ps)\n",
    "    \n",
    "def rprecision(ids, rels):\n",
    "    num_good = np.sum(list(rels.values()))\n",
    "    return precision(num_good, ids, rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = ms.Mystem()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(a):\n",
    "    lm = ''.join(mystem.lemmatize(a))\n",
    "    lm = ' '.join([(wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i)) for i,j in pos_tag(list(map(lambda x: x.lower(), word_tokenize(lm))))])\n",
    "    return lm\n",
    "\n",
    "def calc_stats(index_name, lemmatize_que=False):\n",
    "    inf = open('../relevant_table_2009.xml')\n",
    "    s = '\\n'.join(inf.readlines())\n",
    "    soup = bs4.BeautifulSoup(s, 'lxml')\n",
    "    \n",
    "    p20 = []\n",
    "    r20 = []\n",
    "    m_ap = []\n",
    "    rp = []\n",
    "    for task in soup.html.body.taskdocumentmatrix.children:\n",
    "        if not isinstance(task, bs4.element.Tag):\n",
    "            continue\n",
    "        query_id = task['id']\n",
    "        ids = search_docid(index_name, query_id)\n",
    "        rels = dict()\n",
    "        for doc in task.children:\n",
    "            if not isinstance(doc, bs4.element.Tag):\n",
    "                continue\n",
    "            doc_id = doc['id']\n",
    "            #if doc_id in indexed:\n",
    "            rel = 1 if (doc['relevance'] == 'vital') else 0\n",
    "            rels[doc_id] = rel\n",
    "        rel_ids = [-1 if i not in rels else rels[i] for i in ids]\n",
    "        p20.append(precision(20, ids, rels))\n",
    "        r20.append(recall(20, ids, rels))\n",
    "        m_ap.append(mean_ap(20, ids, rels))\n",
    "        rp.append(rprecision(ids, rels))\n",
    "    \n",
    "    print(f'index_name = {index_name}')\n",
    "    print(f'p@20 = {np.average(p20)}')\n",
    "    print(f'r@20 = {np.average(r20)}')\n",
    "    print(f'map = {np.average(m_ap)}')\n",
    "    print(f'r-precision = {np.average(rp)}')\n",
    "    \n",
    "    size = es.indices.stats(index_name)['_all']['primaries']['store']['size_in_bytes']\n",
    "    print(f'size = {size}')\n",
    "    \n",
    "    start = time()\n",
    "    it = 0\n",
    "    qids = list(queries.keys())[:200]\n",
    "    for que in qids:\n",
    "        it += 1\n",
    "        if lemmatize_que:\n",
    "            que = lemmatize(que)\n",
    "            print(que)\n",
    "        search_docid(index_name, que)\n",
    "    finish = time()\n",
    "    print(f'query time: {(finish - start) / 200}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_all()\n",
    "start = time()\n",
    "run_generator(pre_no_snowball, preprocessing_generator)\n",
    "end = time()\n",
    "print(f'building time = {end - start}')\n",
    "calc_stats(pre_no_snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57706b4a92041ab834afc18d6fc65c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb849815b404ef4922be59a03713ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building time = 100.38830256462097\n",
      "index_name = pre_snowball\n",
      "p@20 = 0.352769337076931\n",
      "r@20 = 0.2833510481656717\n",
      "map = 0.4619860928688837\n",
      "r-precision = 0.3529810102014603\n",
      "size = 1006910430\n",
      "query time: 0.023696205615997314\n"
     ]
    }
   ],
   "source": [
    "recreate_all()\n",
    "start = time()\n",
    "run_generator(pre_snowball, preprocessing_generator)\n",
    "end = time()\n",
    "print(f'building time = {end - start}')\n",
    "calc_stats(pre_snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aa5d9982784a36956152a9df96fae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a907afd78ebb4365a97ce54ce8e5fa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building time = 115.24400067329407\n",
      "index_name = lemma_no_snowball\n",
      "p@20 = 0.30379587599025454\n",
      "r@20 = 0.2224024488917856\n",
      "map = 0.37690104087900733\n",
      "r-precision = 0.30659024445275884\n",
      "size = 1079551095\n",
      "query time: 0.04361544609069824\n"
     ]
    }
   ],
   "source": [
    "recreate_all()\n",
    "start = time()\n",
    "run_generator(lemma_no_snowball, lemmas_generator)\n",
    "end = time()\n",
    "print(f'building time = {end - start}')\n",
    "calc_stats(lemma_no_snowball, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

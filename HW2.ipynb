{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time\n",
    "import bs4\n",
    "import base64\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_index_name=\"raw_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index(name, settings):\n",
    "    if es.indices.exists(index=name):\n",
    "        es.indices.delete(index=name)\n",
    "    es.indices.create(index=name, body=settings)\n",
    "    \n",
    "def check_analyzer(raw_index_name, analyzer, text):\n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    \n",
    "    tokens = es.indices.analyze(index=raw_index_name, body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_settings = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'id': {\n",
    "                'type': 'integer'\n",
    "            },\n",
    "            'url': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                'fields': {\n",
    "                    'complex': {\n",
    "                        'type': 'text',\n",
    "                        'analyzer': 'russian_complex'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'russian_complex': {\n",
    "                    'char_filter': [\n",
    "                        'no_html'\n",
    "                    ],\n",
    "                    'tokenizer': 'word_longer_2',\n",
    "                    'filter': [\n",
    "                        'lowercase',\n",
    "                        'russian_snow'\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "            'char_filter': {\n",
    "                'no_html': {\n",
    "                    'type': 'html_strip',\n",
    "                    \"escaped_tags\": []\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'word_longer_2': {\n",
    "                    'type': 'pattern',\n",
    "                    'pattern': '[a-zA-Z_0-9\\u0400-\\u04FF]{2,}',\n",
    "                    'group': 0\n",
    "                }\n",
    "            },\n",
    "            'filter': {\n",
    "                'russian_snow': {\n",
    "                    'type': 'snowball',\n",
    "                    'language': 'russian'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_index(raw_index_name, snowball_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мое', 'имя']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '<div><a>Мое имя!!</a></div>'\n",
    "\n",
    "analyzer = {\n",
    "    'char_filter': ['no_html'],\n",
    "    'tokenizer': 'letter',\n",
    "    'filter': ['lowercase']\n",
    "}\n",
    "\n",
    "check_analyzer(raw_index_name, analyzer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_actions_generator(index_name):\n",
    "    index = 0\n",
    "    for doc_id in range(3):\n",
    "        with open(f'../byweb_for_course/byweb.{doc_id}.xml', 'r') as inf:\n",
    "            inf.readline()\n",
    "            for i in tqdm(range(20000)):\n",
    "                index = index + 1\n",
    "                a = inf.readline()\n",
    "                b = inf.readline()\n",
    "                c = inf.readline()\n",
    "                page = a + b + c\n",
    "                doc = {}\n",
    "                document = bs4.BeautifulSoup(page, \"lxml\")\n",
    "                doc['id'] = document.docid.contents[0]\n",
    "                doc['url'] = base64.b64decode(document.docurl.contents[0]).decode(\"cp1251\")\n",
    "                doc['content'] = base64.b64decode(document.content.contents[0]).decode(\"cp1251\")\n",
    "                yield create_es_action(index_name, doc['id'], doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generator(index_name):\n",
    "    for ok, result in parallel_bulk(es, es_actions_generator(index_name), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "        if not ok:\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9181e341827b45acb2f9b58fcffe1cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05724fbdc0ee4c89b0f82074ad3b7159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recreate_index(raw_index_name, snowball_settings)\n",
    "run_generator(raw_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(index, query, *args):\n",
    "    pretty_print_result(es.search(index=index, body=query, size=20), args)\n",
    "    # note that size set to 20 just because default value is 10 and we know that we have 12 docs and 10 < 12 < 20\n",
    "                        \n",
    "def pretty_print_result(search_result, fields=[]):\n",
    "    # fields is a list of fields names which we want to be printed\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "        for field in fields:\n",
    "            print(f'{field}: {hit[\"_source\"][field]}')\n",
    "                  \n",
    "def get_doc_by_id(index, doc_id):\n",
    "    return es.get(index=index, id=doc_id)['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 10000\n",
      "Doc 29001, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=3&i=1622&ic=1282\n",
      "Doc 29002, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=2232&ic=789\n",
      "Doc 29003, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=634&ic=2330\n",
      "Doc 29004, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=779&ic=663\n",
      "Doc 29005, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=2232&ic=1736\n",
      "Doc 29006, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=3&i=1622&ic=1912\n",
      "Doc 29007, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=16&i=126&ic=452\n",
      "Doc 29008, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=9&i=915&ic=430\n",
      "Doc 29009, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=74&ic=570\n",
      "Doc 29010, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=3&i=193&ic=1912\n",
      "Doc 29011, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=660&ic=3714\n",
      "Doc 29012, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=3&i=2179&ic=342\n",
      "Doc 29013, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=3&i=214&ic=716\n",
      "Doc 29014, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=1867&ic=2054\n",
      "Doc 29015, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=1555&ic=575\n",
      "Doc 29016, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=16&i=2350&ic=1945\n",
      "Doc 29017, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=4&i=2089&ic=2439\n",
      "Doc 29018, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=3&i=178&ic=33\n",
      "Doc 29019, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=4&i=2419&ic=980\n",
      "Doc 29020, score is 1.0\n",
      "url: http://www.atom.by/index.php?c=8&i=1639&ic=963\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    'query': {\n",
    "        'match_all': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "search(raw_index_name, query, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries():\n",
    "    inf = open('../web2008_adhoc.xml', 'r', encoding='cp1251')\n",
    "    for _ in range(9):\n",
    "        inf.readline()\n",
    "    \n",
    "    queries = dict()\n",
    "    for line in inf.readlines()[:-1]:\n",
    "        query_text = re.split('<.*?>', line)[2]\n",
    "        query_id = line[10:18]\n",
    "        queries[query_id] = query_text\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_match_query(query):\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'must': {\n",
    "                    'match': {\n",
    "                        'content': query\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return query\n",
    "\n",
    "def search_docid(index, query):\n",
    "    query = create_match_query(query)\n",
    "    res = es.search(index=index, body=query, size=300)\n",
    "    hits = res['hits']['hits']\n",
    "    return [doc['_source']['id'] for doc in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relevance_soup():\n",
    "    inf = open('../relevant_table_2009.xml')\n",
    "    s = '\\n'.join(inf.readlines())\n",
    "    return bs4.BeautifulSoup(s, 'lxml')\n",
    "\n",
    "def precision(k, ids, rels):\n",
    "    ids = ids[:k]\n",
    "    if len(ids) == 0:\n",
    "        return 0\n",
    "    top = 0\n",
    "    good = 0\n",
    "    for i in ids:\n",
    "        if i in rels:\n",
    "            top += 1\n",
    "            good += rels[i]\n",
    "    if top == 0:\n",
    "        return 0\n",
    "    return good / top\n",
    "\n",
    "def recall(k, ids, rels):\n",
    "    ids = ids[:k]\n",
    "    if len(ids) == 0:\n",
    "        return 0\n",
    "    good = 0\n",
    "    num_good = np.sum(list(rels.values()))\n",
    "    relev = min(num_good, k)\n",
    "    for i in ids:\n",
    "        if i in rels:\n",
    "            good += rels[i]\n",
    "    if relev == 0:\n",
    "        return 0\n",
    "    return good / relev\n",
    "\n",
    "def mean_ap(k, ids, rels):\n",
    "    ids = ids[:k]\n",
    "    ps = []\n",
    "    for i in ids:\n",
    "        if i in rels:\n",
    "            ps.append(rels[i])\n",
    "    if np.sum(ps) == 0:\n",
    "        return 0\n",
    "    sums = np.cumsum(ps) / (np.array(range(len(ps))) + 1)\n",
    "    return np.sum(sums * ps) / np.sum(ps)\n",
    "    \n",
    "def rprecision(ids, rels):\n",
    "    num_good = np.sum(list(rels.values()))\n",
    "    return precision(num_good, ids, rels)\n",
    "\n",
    "def run_queries(index, queries, relevance_soup):\n",
    "    p20, r20, m_ap, rp = [], [], [], []\n",
    "    for task in tqdm_notebook(relevance_soup.html.body.taskdocumentmatrix.children):\n",
    "        if not isinstance(task, bs4.element.Tag):\n",
    "            continue\n",
    "        query_id = task['id']\n",
    "        if not(query_id in queries):\n",
    "            continue\n",
    "        ids = search_docid(index, queries[query_id])\n",
    "        rels = dict()\n",
    "        for doc in task.children:\n",
    "            if not isinstance(doc, bs4.element.Tag):\n",
    "                continue\n",
    "            doc_id = doc['id']\n",
    "            rel = 1 if (doc['relevance'] == 'vital') else 0\n",
    "            rels[doc_id] = rel\n",
    "        p20.append(precision(20, ids, rels))\n",
    "        r20.append(recall(20, ids, rels))\n",
    "        m_ap.append(mean_ap(20, ids, rels))\n",
    "        rp.append(rprecision(ids, rels))\n",
    "    return { \n",
    "        'precision': np.average(p20), \n",
    "        'recall': np.average(r20), \n",
    "        'map_20': np.average(m_ap), \n",
    "        'r-precision': np.average(rp)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6959829b9ba407dbe6b8a720d434388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = get_queries()\n",
    "relevance_soup = build_relevance_soup()\n",
    "run_queries('raw_index', queries, relevance_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
